/**
 * RAG Retrieval Module — BM25-based search over the knowledge base index.
 *
 * Loads the pre-computed index (generated by scripts/chunk-documents.ts) and
 * retrieves the top-k most relevant chunks for a given query. The retrieved
 * text is injected as context into the LLM prompt.
 *
 * BM25 is chosen over embedding-based search because:
 * - Zero extra model required (no embedding model to load)
 * - Instant search (<1ms for 155 chunks)
 * - Works perfectly offline with no GPU
 * - For structured, domain-specific text, BM25 is competitive with embeddings
 */

import { readFileSync } from "node:fs";
import { join } from "node:path";

// --- Types ---
interface Chunk {
  id: string;
  text: string;
  domain: string;
  section: string;
  source: string;
  tokens: string[];
}

interface KnowledgeIndex {
  version: number;
  created: string;
  totalChunks: number;
  avgDocLen: number;
  corpusSize: number;
  df: Record<string, number>;
  chunks: Chunk[];
}

export interface RetrievalResult {
  text: string;
  domain: string;
  section: string;
  score: number;
}

// --- Stopwords (same set as chunker) ---
const STOPWORDS = new Set([
  "a", "an", "the", "and", "or", "but", "in", "on", "at", "to", "for",
  "of", "with", "by", "from", "is", "are", "was", "were", "be", "been",
  "being", "have", "has", "had", "do", "does", "did", "will", "would",
  "could", "should", "may", "might", "can", "shall", "this", "that",
  "these", "those", "it", "its", "you", "your", "we", "our", "they",
  "their", "he", "she", "his", "her", "as", "if", "not", "no", "so",
  "up", "out", "about", "into", "than", "then", "also", "just", "more",
  "some", "any", "all", "each", "every", "very", "most", "much", "such",
  "only", "own", "same", "other", "both", "few", "many", "how", "what",
  "which", "who", "when", "where", "why", "there", "here",
]);

/** BM25 parameters */
const K1 = 1.5;
const B = 0.75;

export class RAGRetriever {
  private index: KnowledgeIndex | null = null;
  private loaded = false;

  /**
   * Load the knowledge base index from disk.
   * Call once at startup. Returns false if index file not found.
   */
  load(): boolean {
    try {
      const indexPath = join(
        import.meta.dir,
        "../../data/knowledge/index.json"
      );
      const raw = readFileSync(indexPath, "utf-8");
      this.index = JSON.parse(raw) as KnowledgeIndex;
      this.loaded = true;
      return true;
    } catch (e) {
      console.error("RAG: Failed to load knowledge index:", e);
      this.loaded = false;
      return false;
    }
  }

  /** Whether the index is loaded and ready */
  get isReady(): boolean {
    return this.loaded && this.index !== null;
  }

  /** Number of chunks in the index */
  get chunkCount(): number {
    return this.index?.totalChunks ?? 0;
  }

  /**
   * Tokenise a query string (same process as the chunker)
   */
  private tokenise(text: string): string[] {
    return text
      .toLowerCase()
      .replace(/[^a-z0-9\s-]/g, " ")
      .split(/\s+/)
      .filter((w) => w.length >= 2 && !STOPWORDS.has(w));
  }

  /**
   * Compute BM25 score for a single chunk against query tokens.
   *
   * BM25(D, Q) = Σ IDF(qi) × (f(qi, D) × (k1 + 1)) / (f(qi, D) + k1 × (1 - b + b × |D|/avgdl))
   */
  private bm25Score(chunk: Chunk, queryTokens: string[]): number {
    if (!this.index) return 0;

    const { avgDocLen, corpusSize, df } = this.index;
    const docLen = chunk.tokens.length;

    // Build term frequency map for this chunk
    const tf: Record<string, number> = {};
    for (const token of chunk.tokens) {
      tf[token] = (tf[token] || 0) + 1;
    }

    let score = 0;

    for (const qt of queryTokens) {
      const termDf = df[qt] || 0;
      if (termDf === 0) continue;

      // IDF: log((N - n + 0.5) / (n + 0.5) + 1)
      const idf = Math.log(
        (corpusSize - termDf + 0.5) / (termDf + 0.5) + 1
      );

      // Term frequency in this document
      const termTf = tf[qt] || 0;

      // BM25 term score
      const tfNorm =
        (termTf * (K1 + 1)) /
        (termTf + K1 * (1 - B + B * (docLen / avgDocLen)));

      score += idf * tfNorm;
    }

    return score;
  }

  /**
   * Retrieve the top-k most relevant chunks for a query.
   */
  retrieve(query: string, topK: number = 3): RetrievalResult[] {
    if (!this.index) return [];

    const queryTokens = this.tokenise(query);
    if (queryTokens.length === 0) return [];

    // Score all chunks
    const scored = this.index.chunks.map((chunk) => ({
      chunk,
      score: this.bm25Score(chunk, queryTokens),
    }));

    // Sort by score descending, take top-k
    scored.sort((a, b) => b.score - a.score);

    return scored
      .slice(0, topK)
      .filter((s) => s.score > 0)
      .map((s) => ({
        text: s.chunk.text,
        domain: s.chunk.domain,
        section: s.chunk.section,
        score: s.score,
      }));
  }

  /**
   * Retrieve and format context for injection into the LLM prompt.
   * Returns a formatted string ready to use as the `context` parameter
   * in LLMClient.stream().
   */
  getContext(query: string, topK: number = 3): string | undefined {
    const results = this.retrieve(query, topK);
    if (results.length === 0) return undefined;

    const sections = results.map(
      (r) => r.text.replace(/^\[.*?\]\n?/, "")
    );

    return sections.join("\n\n---\n\n");
  }
}

/**
 * Singleton instance — import and use directly.
 */
export const rag = new RAGRetriever();
